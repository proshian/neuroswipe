{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "from multiprocessing import cpu_count\n",
    "from typing import List, Iterable, Set, Optional, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import SwipeCurveTransformer, get_m1_model, get_m1_bigger_model, get_m1_smaller_model\n",
    "from tokenizers import CharLevelTokenizerv2, KeyboardTokenizerv1\n",
    "from tokenizers import ALL_CYRILLIC_LETTERS_ALPHABET_ORD\n",
    "from dataset import CurveDataset, CurveDatasetSubset\n",
    "from word_generators import GreedyGenerator, BeamGenerator\n",
    "from predict import Predictor\n",
    "from metrics import get_mmr\n",
    "from aggregate_predictions import (separate_out_vocab_all_crvs,\n",
    "                                   append_preds,\n",
    "                                   create_submission,\n",
    "                                   merge_default_and_extra_preds)\n",
    "from nearest_key_lookup import ExtendedNearestKeyLookup\n",
    "from transforms import FullTransform, InitTransform, GetItemTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_KAGGLE = False\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    DATA_ROOT = \"/kaggle/input/yandex-cup-playground\"\n",
    "    MODELS_DIR = \"\"\n",
    "else:\n",
    "    DATA_ROOT = \"../data/data_separated_grid\"\n",
    "    MODELS_ROOT = \"../data/trained_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_PATH = os.path.join(DATA_ROOT, \"valid__in_train_format.jsonl\")\n",
    "TEST_PATH = os.path.join(DATA_ROOT, \"test.jsonl\")\n",
    "\n",
    "VOCAB_PATH = os.path.join(DATA_ROOT, \"voc.txt\")\n",
    "G_NAME_TO_GRID_PATH = os.path.join(DATA_ROOT, \"gridname_to_grid.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(grid_name: str, grids_path: str) -> dict:\n",
    "    with open(grids_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)[grid_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gridname_to_out_of_bounds_coords_dict(\n",
    "        data_paths: List[str], gridname_to_wh: dict,\n",
    "        totals: Iterable[Optional[int]] = None\n",
    "        ) -> Dict[str, Set[Tuple[int, int]]]:\n",
    "    \"\"\"\n",
    "    Returns a dictionary with grid names as keys and lists of out of bounds coordinates as values.\n",
    "    \"\"\"\n",
    "    totals = totals or [None] * len(data_paths)\n",
    "    \n",
    "    gname_to_out_of_bounds = {gname: set() for gname in gridname_to_wh.keys()}\n",
    "\n",
    "    for data_path, total in zip(data_paths, totals):\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "            for line in tqdm(json_file, total=total):\n",
    "                json_data = json.loads(line)\n",
    "                curve = json_data['curve']\n",
    "                grid_name = curve['grid_name']\n",
    "                w, h = gridname_to_wh[grid_name]\n",
    "                X, Y = curve['x'], curve['y']\n",
    "                out_of_bounds = set((x, y) for x, y in zip(X, Y) \n",
    "                                    if x < 0 or x >= w or y < 0 or y >= h)\n",
    "                gname_to_out_of_bounds[grid_name].update(out_of_bounds)\n",
    "    return gname_to_out_of_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(grid_name_to_grid_path: str,\n",
    "                 data_paths: Iterable[str], totals: Iterable[Optional[int]],\n",
    "                 nearest_key_candidates: Set[str],\n",
    "                 kb_tokenizer: KeyboardTokenizerv1,\n",
    "                 word_char_tokenizer: CharLevelTokenizerv2,\n",
    "                 is_full_transform: bool,\n",
    "                 store_gnames: bool\n",
    "                 ) -> List[CurveDataset]:\n",
    "    \n",
    "    grid_names = ('default', 'extra')\n",
    "    gridname_to_grid  = {gname: get_grid(gname, grid_name_to_grid_path) \n",
    "                         for gname in grid_names}\n",
    "\n",
    "    gname_to_wh = {\n",
    "        gname: (grid['width'], grid['height']) \n",
    "        for gname, grid in gridname_to_grid.items()\n",
    "    }\n",
    "    \n",
    "    print(\"Accumulating out-of-bounds coordinates...\")\n",
    "    gname_to_out_of_bounds = get_gridname_to_out_of_bounds_coords_dict(\n",
    "        data_paths, gname_to_wh, totals=totals\n",
    "    )\n",
    "    \n",
    "    print(\"Creating ExtendedNearestKeyLookups...\")\n",
    "    gridname_to_nkl = {\n",
    "        gname: ExtendedNearestKeyLookup(grid, nearest_key_candidates, gname_to_out_of_bounds[gname])\n",
    "        for gname, grid in gridname_to_grid.items()\n",
    "    }\n",
    "\n",
    "    if is_full_transform:\n",
    "        init_transform = FullTransform(\n",
    "            grid_name_to_nk_lookup=gridname_to_nkl,\n",
    "            grid_name_to_wh=gname_to_wh,\n",
    "            kb_tokenizer=kb_tokenizer,\n",
    "            word_tokenizer=word_char_tokenizer,\n",
    "            include_time=False,\n",
    "            include_velocities=True,\n",
    "            include_accelerations=True\n",
    "        )\n",
    "\n",
    "        get_item_transform = None\n",
    "    \n",
    "    else:\n",
    "        init_transform = InitTransform(\n",
    "            grid_name_to_nk_lookup=gridname_to_nkl,\n",
    "            kb_tokenizer=kb_tokenizer,\n",
    "        )\n",
    "\n",
    "        get_item_transform = GetItemTransform(\n",
    "            grid_name_to_wh=gname_to_wh,\n",
    "            word_tokenizer=word_char_tokenizer,\n",
    "            include_time=False,\n",
    "            include_velocities=True,\n",
    "            include_accelerations=True,\n",
    "        )\n",
    "    \n",
    "    print(\"Creating datasets...\")\n",
    "    datasets = []\n",
    "    for d_path, total in zip(data_paths, totals):\n",
    "        ds = CurveDataset(\n",
    "            data_path=d_path,\n",
    "            store_gnames = store_gnames,\n",
    "            init_transform=init_transform,\n",
    "            get_item_transform=get_item_transform,\n",
    "            total = total,\n",
    "        )\n",
    "        datasets.append(ds)\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating out-of-bounds coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 17855.97it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 18743.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ExtendedNearestKeyLookups...\n",
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 884.43it/s]\n",
      "100%|██████████| 10000/10000 [00:10<00:00, 984.38it/s]\n"
     ]
    }
   ],
   "source": [
    "kb_tokenizer = KeyboardTokenizerv1()\n",
    "word_char_tokenizer = CharLevelTokenizerv2(VOCAB_PATH)\n",
    "\n",
    "data_paths = [\n",
    "    VAL_PATH,\n",
    "    TEST_PATH\n",
    "]\n",
    "totals = [10_000, 10_000]\n",
    "\n",
    "\n",
    "# train_dataset, val_dataset = get_datasets(\n",
    "val_dataset, test_dataset = get_datasets(\n",
    "    grid_name_to_grid_path=G_NAME_TO_GRID_PATH,\n",
    "    data_paths = data_paths, totals = totals,\n",
    "    nearest_key_candidates = ALL_CYRILLIC_LETTERS_ALPHABET_ORD,\n",
    "    kb_tokenizer=kb_tokenizer,\n",
    "    word_char_tokenizer=word_char_tokenizer,\n",
    "    is_full_transform = True,\n",
    "    store_gnames = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_default_dataset = CurveDatasetSubset(val_dataset, \"default\")\n",
    "val_extra_dataset = CurveDatasetSubset(val_dataset, \"extra\")\n",
    "\n",
    "test_default_dataset = CurveDatasetSubset(test_dataset, \"default\")\n",
    "test_extra_dataset = CurveDatasetSubset(test_dataset, \"extra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(dataset: CurveDataset) -> List[str]:\n",
    "    targets = []\n",
    "    for _, target_tokens in dataset:\n",
    "        target = word_char_tokenizer.decode(target_tokens[:-1])\n",
    "        targets.append(target)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_set(vocab_path: str):\n",
    "    with open(vocab_path, 'r', encoding = \"utf-8\") as f:\n",
    "        return set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = get_vocab_set(os.path.join(DATA_ROOT, \"voc.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\proshian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "grid_name = \"default\"\n",
    "model_getter = get_m1_bigger_model\n",
    "weights_path = os.path.join(MODELS_ROOT, \"m1_bigger/m1_bigger_v2__2023_11_12__12_30_29__0.13121__greed_acc_0.86098__default_l2_0_ls0_switch_2.pt\")\n",
    "model = model_getter(device, weights_path)\n",
    "grid_name_to_greedy_generator = {grid_name: GreedyGenerator(model, word_char_tokenizer, device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target               prediction           prob                \n",
      "-------------------------------------------------\n",
      "на                   на                   0.99890             \n",
      "все                  все                  0.99983             \n",
      "добрый               добрый               0.99704             \n",
      "девочка              девочка              0.99489             \n",
      "сказала              сказала              0.99733             \n",
      "скинь                скинь                0.99880             \n",
      "геев                 геев                 0.65820             \n",
      "тобой                тобой                0.99947             \n",
      "была                 бысса                0.28788             \n",
      "да                   да                   0.99748             \n",
      "муж                  муж                  0.91975             \n",
      "щас                  щас                  0.99749             \n",
      "она                  она                  0.99797             \n",
      "проблема             проблема             0.98953             \n",
      "билайн               билайн               0.83472             \n",
      "уже                  уже                  0.99979             \n",
      "раньше               раньше               0.99143             \n",
      "рам                  рам                  0.47685             \n",
      "щас                  щас                  0.99957             \n",
      "купил                купил                0.96394             \n",
      "ты                   ты                   0.99967             \n",
      "зовут                зовут                0.99671             \n",
      "короче               короче               0.99829             \n",
      "размыто              размыто              0.44333             \n",
      "давай                давай                0.98772             \n",
      "отдать               отжать               0.60423             \n",
      "привет               привет               0.99959             \n",
      "не                   не                   0.99116             \n",
      "да                   да                   0.99934             \n",
      "будете               будете               0.99785             \n",
      "связи                связи                0.99097             \n"
     ]
    }
   ],
   "source": [
    "greedy_generator = GreedyGenerator(model, word_char_tokenizer, device)\n",
    "\n",
    "\n",
    "print(\"{:<20} {:<20} {:<20}\".format(\"target\", \"prediction\", \"prob\"))\n",
    "print(\"-\"*49)\n",
    "\n",
    "n_examples = 30\n",
    "\n",
    "for i, data in enumerate(val_default_dataset):\n",
    "\n",
    "    (xyt, kb_tokens, dec_in_char_seq), target = data\n",
    "\n",
    "    score, pred = greedy_generator(xyt, kb_tokens)[0]\n",
    "\n",
    "    target = word_char_tokenizer.decode(target[:-1])\n",
    "    print(\"{:<20} {:<20} {:<20.5f}\".format(target, pred, np.exp(score)))\n",
    "\n",
    "    if i >= n_examples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target               prediction           prob                \n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\proshian\\Documents\\yandex-cup-2023-ml-neuroswipe\\src\\word_generators.py:115: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  next_tokens_logproba = F.log_softmax(next_tokens_logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "на                   на                   1.00096             \n",
      "все                  все                  1.00015             \n",
      "добрый               добрый               1.00278             \n",
      "девочка              девочка              1.00484             \n",
      "сказала              сказала              1.00253             \n",
      "скинь                скинь                1.00111             \n",
      "геев                 геев                 1.46492             \n",
      "тобой                тобой                1.00049             \n",
      "была                 бысса                3.16716             \n",
      "да                   да                   1.00219             \n",
      "муж                  муж                  1.07770             \n"
     ]
    }
   ],
   "source": [
    "beam_generator = BeamGenerator(model, word_char_tokenizer, device)\n",
    "\n",
    "print(\"{:<20} {:<20} {:<20}\".format(\"target\", \"prediction\", \"prob\"))\n",
    "print(\"-\"*49)\n",
    "\n",
    "n_examples = 10\n",
    "\n",
    "for i, data in enumerate(val_default_dataset):\n",
    "\n",
    "    normalization_factor = 0.5\n",
    "\n",
    "    (xyt, kb_tokens, dec_in_char_seq), target = data\n",
    "\n",
    "    score, pred = beam_generator(xyt, kb_tokens, max_steps_n = 35, \n",
    "        normalization_factor=normalization_factor)[0]\n",
    "\n",
    "    target = word_char_tokenizer.decode(target[:-1])\n",
    "\n",
    "    denorm_score = score * (len(pred) + 1)**normalization_factor\n",
    "\n",
    "    print(\"{:<20} {:<20} {:<20.5f}\".format(target, pred, np.exp(denorm_score)))\n",
    "\n",
    "    if i >= n_examples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вероятности больше 1... Что-то в beamsearch напутано с нормализацией, потому что после ее устранения все равно больше 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_default_targets = get_targets(val_default_dataset)\n",
    "val_extra_targets = get_targets(val_extra_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models separately and as a pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_default_targets = get_targets(val_default_dataset)\n",
    "val_extra_targets = get_targets(val_extra_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_predictions = weights_to_raw_predictions(\n",
    "    grid_name = \"default\",\n",
    "    model_getter=get_m1_bigger_model,\n",
    "    weights_path = os.path.join(MODELS_ROOT, \"m1_bigger/m1_bigger_v2__2023_11_12__12_30_29__0.13121__greed_acc_0.86098__default_l2_0_ls0_switch_2.pt\"),\n",
    "    word_char_tokenizer=word_char_tokenizer,\n",
    "    dataset=val_default_dataset,\n",
    "    generator_ctor=GreedyGenerator,\n",
    "    n_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8531223449447749"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_MMR =  get_mmr(default_predictions, val_default_targets)\n",
    "default_MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_predictions_best_bigger = default_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_predictions_best_bigger_clean, _ = separate_out_vocab_all_crvs(default_predictions_best_bigger, vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8784"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bool(el) for el in default_predictions_best_bigger_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9416/9416 [04:08<00:00, 37.90it/s]\n"
     ]
    }
   ],
   "source": [
    "default_predictions = weights_to_raw_predictions(\n",
    "    grid_name = \"default\",\n",
    "    model_getter=get_m1_smaller_model,\n",
    "    weights_path = os.path.join(MODELS_ROOT, \"m1_smaller/m1_smaller_v2_2023_11_11_17_43_35_0_33179_default_l2_1e_05_ls0_02.pt\"),\n",
    "    word_char_tokenizer=word_char_tokenizer,\n",
    "    dataset=val_default_dataset,\n",
    "    generator_ctor=GreedyGenerator,\n",
    "    n_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_predictions_clean, _ = separate_out_vocab_all_crvs(default_predictions, vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8449"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bool(el) for el in default_predictions_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8221112999150383"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_MMR =  get_mmr(default_predictions, val_default_targets)\n",
    "default_MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_name = \"default\"\n",
    "# model_getter = get_m1_bigger_model\n",
    "# weights_path = os.path.join(MODELS_ROOT, \"m1_bigger/m1_bigger_v2__2023_11_11__13_17_50__0.13845_default_l2_0_ls0_switch_0.pt\")\n",
    "# model = model_getter(device, weights_path)\n",
    "# grid_name_to_greedy_generator = {grid_name: GreedyGenerator(model, word_char_tokenizer, device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9416/9416 [05:02<00:00, 31.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# default_predictions = predict_greedy_raw_multiproc(val_default_dataset,\n",
    "#                                                     grid_name_to_greedy_generator,\n",
    "#                                                     num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_name = \"extra\"\n",
    "model_getter = get_m1_model\n",
    "weights_path = os.path.join(MODELS_ROOT, \"m1_v2/m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt\")\n",
    "model = model_getter(device, weights_path)\n",
    "grid_name_to_greedy_generator = {grid_name: GreedyGenerator(model, word_char_tokenizer, device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 584/584 [00:18<00:00, 31.60it/s]\n"
     ]
    }
   ],
   "source": [
    "extra_predictions = predict_raw_mp(val_extra_dataset,\n",
    "                                   grid_name_to_greedy_generator,\n",
    "                                   num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.851027397260274"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_MMR = get_mmr(extra_predictions, val_extra_targets)\n",
    "extra_MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = merge_default_and_extra_preds(default_predictions, extra_predictions, val_default_dataset.grid_name_idxs, val_extra_dataset.grid_name_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = None\n",
    "with open(os.path.join(DATA_ROOT, \"valid.ref\"), 'r', encoding='utf-8') as f:\n",
    "    all_targets = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8512"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_MMR = get_mmr(all_preds, all_targets)\n",
    "full_MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(el) for el in all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"m1_v2/best_model__2023_11_09__10_36_02__0.14229_default_switch_2_try_2.pt\": 0.8512107051826678,\n",
    " \"m1_v2/m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt\": 0.851027397260274,\n",
    " \n",
    " \"m1_bigger/m1_bigger_v2__2023_11_10__13_38_32__0.50552_default_l2_5e-05_ls0.045_switch_0.pt\": 0.810429056924384,\n",
    " \"m1_bigger/m1_bigger_v2__2023_11_10__16_36_38__0.49848_default_l2_5e-05_ls0.045_switch_0.pt\": 0.818500424808836,\n",
    " \"m1_bigger/m1_bigger_v2__2023_11_10__21_51_01__0.49382_default_l2_5e-05_ls0.045_switch_0.pt\": 0.8210492778249787,\n",
    " \n",
    " \"m1_bigger/m1_bigger_v2__2023_11_11__13_17_50__0.13845_default_l2_0_ls0_switch_0.pt\": 0.8512107051826678,\n",
    " \"m1_bigger/m1_bigger_v2__2023_11_11__14_29_37__0.13679_default_l2_0_ls0_switch_0.pt\": 0.8531223449447749,\n",
    " \n",
    " \"m1_smaller/m1_smaller_v2_2023_11_11_17_43_35_0_33179_default_l2_1e_05_ls0_02.pt\": 0.8221112999150383}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create a greedy submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_name = \"default\"\n",
    "model_getter = get_m1_model\n",
    "weights_path = os.path.join(MODELS_ROOT, \"m1_v2/m1_v2__2023_11_09__10_36_02__0.14229_default_switch_0.pt\")\n",
    "model = model_getter(device, weights_path)\n",
    "grid_name_to_greedy_generator = {grid_name: GreedyGenerator(model, word_char_tokenizer, device)}\n",
    "default_test_predictions = predict_raw_mp(test_default_dataset,\n",
    "                                          grid_name_to_greedy_generator,\n",
    "                                          num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 627/627 [00:20<00:00, 30.18it/s]\n"
     ]
    }
   ],
   "source": [
    "grid_name = \"extra\"\n",
    "model_getter = get_m1_model\n",
    "weights_path = os.path.join(MODELS_ROOT, \"m1_v2/m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt\")\n",
    "model = model_getter(device, weights_path)\n",
    "grid_name_to_greedy_generator = {grid_name: GreedyGenerator(model, word_char_tokenizer, device)}\n",
    "extra_test_predictions = predict_raw_mp(test_extra_dataset,\n",
    "                                        grid_name_to_greedy_generator,\n",
    "                                        num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = merge_default_and_extra_preds(default_test_predictions, extra_test_predictions, test_default_dataset.grid_name_idxs, test_extra_dataset.grid_name_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = get_vocab_set(os.path.join(DATA_ROOT, \"voc.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_preds, invalid_test_preds = separate_out_vocab_all_crvs(all_test_preds, vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['на'],\n",
       " ['что'],\n",
       " ['опоздания'],\n",
       " ['сколько'],\n",
       " [],\n",
       " ['не'],\n",
       " ['как'],\n",
       " ['садовод'],\n",
       " ['заметил'],\n",
       " ['ваги']]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_list = None\n",
    "with open(r\"..\\data\\submissions\\sample_submission.csv\", 'r', encoding = 'utf-8') as f:\n",
    "    augment_lines = f.read().splitlines()\n",
    "augment_list = [line.split(\",\") for line in augment_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_test_preds = append_preds(clean_test_preds, augment_list, limit = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['на', 'неа', 'на', 'ненка'],\n",
       " ['что', 'часто', 'частого', 'чисто'],\n",
       " ['опоздания', 'опоздания', 'опозданиям', 'оприходования'],\n",
       " ['сколько', 'сколько', 'сокольского', 'свердловского'],\n",
       " ['дремать', 'дописать', 'донимать', 'дюрренматт'],\n",
       " ['не', 'неук', 'нк', 'ненка'],\n",
       " ['как', 'как', 'капак', 'капе'],\n",
       " ['садовод', 'спародировал', 'садовод', 'сурдоперевод'],\n",
       " ['заметил', 'знаменито', 'знаменитого', 'замерил'],\n",
       " ['ваги', 'ваенги', 'венгрии', 'ванги']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_test_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_name = \"m1_v2__0.14229_deault__0.14301_extra__greedy.csv\"\n",
    "out_path = rf\"..\\data\\submissions\\{submission_name}\"\n",
    "create_submission(augmented_test_preds, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeamSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "def remove_beamsearch_probs(preds: List[List[Tuple[float, str]]]) -> List[List[str]]:\n",
    "    new_preds = []\n",
    "    for pred_line in preds:\n",
    "        new_preds_line = []\n",
    "        for _, word in pred_line:\n",
    "            new_preds_line.append(word)\n",
    "        new_preds.append(new_preds_line)\n",
    "    return new_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_wrong_prediction_shape(prediciton):\n",
    "    return [pred_el[0] for pred_el in prediciton]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beamsearch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VAL_WORD_LEN = max(len(el) for el in all_targets)\n",
    "\n",
    "generator_kwargs = {\n",
    "    'max_steps_n': MAX_VAL_WORD_LEN+1,\n",
    "    'return_hypotheses_n': 7,\n",
    "    'beamsize': 6,\n",
    "    'normalization_factor': 0.5,\n",
    "    'verbose': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_name_to_val_dataset = {\n",
    "    'default': val_default_dataset,\n",
    "    'extra': val_extra_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9416/9416 [41:23<00:00,  3.79it/s] \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "bs_params = [\n",
    "    (\"default\", get_m1_bigger_model, \"m1_bigger/m1_bigger_v2__2023_11_12__14_51_49__0.13115__greed_acc_0.86034__default_l2_0_ls0_switch_2.pt\"),\n",
    "]\n",
    "\n",
    "\n",
    "for grid_name, model_getter, weights_f_name in bs_params:\n",
    "\n",
    "    bs_preds_path = os.path.join(\"../data/saved_beamsearch_validation_results/\",\n",
    "                                f\"{weights_f_name.replace('/', '__')}.pkl\")\n",
    "    \n",
    "    if os.path.exists(bs_preds_path):\n",
    "        print(f\"Path {bs_preds_path} exists. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    bs_predictions = weights_to_raw_predictions(\n",
    "        grid_name = grid_name,\n",
    "        model_getter=model_getter,\n",
    "        weights_path = os.path.join(MODELS_ROOT, weights_f_name),\n",
    "        word_char_tokenizer=word_char_tokenizer,\n",
    "        dataset=grid_name_to_val_dataset[grid_name],\n",
    "        generator_ctor=BeamGenerator,\n",
    "        n_workers=4,\n",
    "        generator_kwargs=generator_kwargs\n",
    "    )\n",
    "\n",
    "    with open(bs_preds_path, 'wb') as f:\n",
    "        pickle.dump(bs_predictions, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8929800339847141"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_name = \"m1_bigger__m1_bigger_v2__2023_11_12__14_51_49__0.13115__greed_acc_0.86034__default_l2_0_ls0_switch_2.pt.pkl\"\n",
    "bs_preds_path = os.path.join(\"../data/saved_beamsearch_validation_results/\",\n",
    "                                preds_name)\n",
    "with open(bs_preds_path, 'rb') as f:\n",
    "    default_valid_preds_bs = pickle.load(f)\n",
    "\n",
    "default_valid_preds_bs = patch_wrong_prediction_shape(default_valid_preds_bs)\n",
    "default_valid_preds_bs = remove_beamsearch_probs(default_valid_preds_bs)\n",
    "default_valid_preds_bs, _ = separate_out_vocab_all_crvs(default_valid_preds_bs, vocab_set)\n",
    "get_mmr(default_valid_preds_bs, val_default_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8876027397260277"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_name = \"m1_v2__m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt.pkl\"\n",
    "bs_preds_path = os.path.join(\"../data/saved_beamsearch_validation_results/\",\n",
    "                                preds_name)\n",
    "with open(bs_preds_path, 'rb') as f:\n",
    "    extra_valid_preds_bs = pickle.load(f)\n",
    "\n",
    "extra_valid_preds_bs = patch_wrong_prediction_shape(extra_valid_preds_bs)\n",
    "extra_valid_preds_bs = remove_beamsearch_probs(extra_valid_preds_bs)\n",
    "extra_valid_preds_bs, _ = separate_out_vocab_all_crvs(extra_valid_preds_bs, vocab_set)\n",
    "get_mmr(extra_valid_preds_bs, val_extra_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#     \"m1_smaller__m1_smaller_v2_2023_11_12_01_21_45_0_31891_default_l2_1e_05_ls0_02.pt.pkl\": 0.8811533559898118,\n",
    "#     \"m1_smaller__m1_smaller_v2_2023_11_12_08_17_33_0_31223_default_l2_1e_05_ls0_02.pt.pkl\": 0.8835960067969484,\n",
    "#     \"m1_bigger__m1_bigger_v2__2023_11_11__22_18_35__0.13542_default_l2_0_ls0_switch_1.pt.pkl\": 0.8900881478334822,\n",
    "#     \"m1_bigger__m1_bigger_v2__2023_11_11__15_53_07__0.13636_default_l2_0_ls0_switch_0.pt.pkl\": 0.8871590909090984,\n",
    "#     \"m1_bigger__m1_bigger_v2__2023_11_12__00_39_33__0.13297_default_l2_0_ls0_switch_1.pt.pkl\": 0.887674171622777,\n",
    "#     \"m1_v2__m1_v2__2023_11_09__10_36_02__0.14229_default_switch_0.pt.pkl\": 0.8877740016992428,\n",
    "    \n",
    "    \n",
    "#     \"m1_bigger__m1_bigger_v2__2023_11_11__16_45_33__0.13721_extra_l2_0_ls0_switch_0.pt.pkl\": 0.8864383561643838,\n",
    "#     \"m1_v2__m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt.pkl\": 0.8876027397260277\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_name_to_ranged_bs_model_preds_paths = {\n",
    "    'default': [\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__14_51_49__0.13115__greed_acc_0.86034__default_l2_0_ls0_switch_2.pt.pkl\", #: 0.8929800339847141,\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__12_30_29__0.13121__greed_acc_0.86098__default_l2_0_ls0_switch_2.pt.pkl\", #: 0.8914698385726496,\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__22_18_35__0.13542_default_l2_0_ls0_switch_1.pt.pkl\",  #: 0.8900881478334822,\n",
    "        \"m1_v2__m1_v2__2023_11_09__10_36_02__0.14229_default_switch_0.pt.pkl\",  #: E 0.8877740016992428,\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__00_39_33__0.13297_default_l2_0_ls0_switch_1.pt.pkl\",  #: 0.887674171622777,\n",
    "        # \"m1_bigger__m1_bigger_v2__2023_11_11__15_53_07__0.13636_default_l2_0_ls0_switch_0.pt.pkl\",  #: 0.8871590909090984,\n",
    "        \"m1_smaller__m1_smaller_v2__2023_11_12__17_40_42__0.30909_default_l2_1e-05_ls0.02_switch_0.pt.pkl\",  # 0.8849384027187835\n",
    "        # \"m1_smaller__m1_smaller_v2_2023_11_12_08_17_33_0_31223_default_l2_1e_05_ls0_02.pt.pkl\",  #: 0.8835960067969484,\n",
    "        # \"m1_smaller__m1_smaller_v2_2023_11_12_01_21_45_0_31891_default_l2_1e_05_ls0_02.pt.pkl\",  #: 0.8811533559898118,\n",
    "        ],\n",
    "    'extra': [\n",
    "        \"m1_v2__m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__16_45_33__0.13721_extra_l2_0_ls0_switch_0.pt.pkl\"\n",
    "        ]\n",
    "}\n",
    "\n",
    "# Отранжированы по качесту beamsearch на валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "default_idxs = val_default_dataset.grid_name_idxs\n",
    "extra_idxs = val_extra_dataset.grid_name_idxs \n",
    "\n",
    "grid_name_to_augmented_preds = {}\n",
    "\n",
    "for grid_name in ('default', 'extra'):\n",
    "    bs_pred_list = []\n",
    "\n",
    "    for f_name in grid_name_to_ranged_bs_model_preds_paths[grid_name]:\n",
    "        f_path = os.path.join(\"../data/saved_beamsearch_validation_results/\", f_name)\n",
    "        with open(f_path, 'rb') as f:\n",
    "            bs_pred_list.append(pickle.load(f))\n",
    "        \n",
    "    bs_pred_list = [patch_wrong_prediction_shape(bs_preds) for bs_preds in bs_pred_list] \n",
    "    bs_pred_list = [remove_beamsearch_probs(bs_preds) for bs_preds in bs_pred_list]\n",
    "    bs_pred_list = [separate_out_vocab_all_crvs(bs_preds, vocab_set)[0] for bs_preds in bs_pred_list]\n",
    "\n",
    "\n",
    "    augmented_preds = bs_pred_list.pop(0)\n",
    "\n",
    "    while bs_pred_list:\n",
    "        augmented_preds = append_preds(augmented_preds, bs_pred_list.pop(0))\n",
    "\n",
    "    grid_name_to_augmented_preds[grid_name] = augmented_preds\n",
    "\n",
    "\n",
    "full_preds = merge_default_and_extra_preds(\n",
    "    grid_name_to_augmented_preds['default'],\n",
    "    grid_name_to_augmented_preds['extra'],\n",
    "    default_idxs,\n",
    "    extra_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "n_preds_in_line_dict = defaultdict(int)\n",
    "\n",
    "for line in full_preds:\n",
    "    n_preds_in_line_dict[len(line)] += 1\n",
    "\n",
    "print(n_preds_in_line_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = None\n",
    "with open(os.path.join(DATA_ROOT, \"valid.ref\"), 'r', encoding='utf-8') as f:\n",
    "    all_targets = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mmr(full_preds, all_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "grid_name_to_ranged_bs_model_preds_paths = {\n",
    "    'default': [\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__22_18_35__0.13542_default_l2_0_ls0_switch_1.pt.pkl\",#: 0.8900881478334822,\n",
    "        \"m1_v2__m1_v2__2023_11_09__10_36_02__0.14229_default_switch_0.pt.pkl\",#: 0.8877740016992428,\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__00_39_33__0.13297_default_l2_0_ls0_switch_1.pt.pkl\",#: 0.887674171622777,\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__15_53_07__0.13636_default_l2_0_ls0_switch_0.pt.pkl\",#: 0.8871590909090984,\n",
    "        \"m1_smaller__m1_smaller_v2_2023_11_12_08_17_33_0_31223_default_l2_1e_05_ls0_02.pt.pkl\",#: 0.8835960067969484,\n",
    "        \"m1_smaller__m1_smaller_v2_2023_11_12_01_21_45_0_31891_default_l2_1e_05_ls0_02.pt.pkl\",#: 0.8811533559898118,\n",
    "        ],\n",
    "    'extra': [\n",
    "        \"m1_v2__m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__16_45_33__0.13721_extra_l2_0_ls0_switch_0.pt.pkl\"\n",
    "        ]\n",
    "}\n",
    "```\n",
    "\n",
    "0.8936010000000082\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "grid_name_to_ranged_bs_model_preds_paths = {\n",
    "    'default': [\n",
    "        \"m1_v2__m1_v2__2023_11_09__10_36_02__0.14229_default_switch_0.pt.pkl\",#: 0.8877740016992428,\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__22_18_35__0.13542_default_l2_0_ls0_switch_1.pt.pkl\",#: 0.8900881478334822,\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__00_39_33__0.13297_default_l2_0_ls0_switch_1.pt.pkl\",#: 0.887674171622777,\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__15_53_07__0.13636_default_l2_0_ls0_switch_0.pt.pkl\",#: 0.8871590909090984,\n",
    "        \"m1_smaller__m1_smaller_v2_2023_11_12_08_17_33_0_31223_default_l2_1e_05_ls0_02.pt.pkl\",#: 0.8835960067969484,\n",
    "        \"m1_smaller__m1_smaller_v2_2023_11_12_01_21_45_0_31891_default_l2_1e_05_ls0_02.pt.pkl\",#: 0.8811533559898118,\n",
    "        ],\n",
    "    'extra': [\n",
    "        \"m1_v2__m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__16_45_33__0.13721_extra_l2_0_ls0_switch_0.pt.pkl\"\n",
    "        ]\n",
    "}\n",
    "```\n",
    "\n",
    "0.892801000000009\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "grid_name_to_ranged_bs_model_preds_paths = {\n",
    "    'default': [\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__22_18_35__0.13542_default_l2_0_ls0_switch_1.pt.pkl\",#: 0.8900881478334822,\n",
    "        \"m1_v2__m1_v2__2023_11_09__10_36_02__0.14229_default_switch_0.pt.pkl\",#: 0.8877740016992428,\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__00_39_33__0.13297_default_l2_0_ls0_switch_1.pt.pkl\",#: 0.887674171622777,\n",
    "        \"m1_smaller__m1_smaller_v2_2023_11_12_08_17_33_0_31223_default_l2_1e_05_ls0_02.pt.pkl\",#: 0.8835960067969484,\n",
    "        \"m1_smaller__m1_smaller_v2_2023_11_12_01_21_45_0_31891_default_l2_1e_05_ls0_02.pt.pkl\",#: 0.8811533559898118,\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__15_53_07__0.13636_default_l2_0_ls0_switch_0.pt.pkl\",#: 0.8871590909090984,\n",
    "        ],\n",
    "    'extra': [\n",
    "        \"m1_v2__m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__16_45_33__0.13721_extra_l2_0_ls0_switch_0.pt.pkl\"\n",
    "        ]\n",
    "}\n",
    "\n",
    "# должны ранжироваться по качесту beamsearch на валидации\n",
    "```\n",
    "\n",
    "0.8936000000000084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beamsearch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LEN = 36\n",
    "\n",
    "generator_kwargs = {\n",
    "    'max_steps_n': MAX_WORD_LEN - 1,\n",
    "    'return_hypotheses_n': 7,\n",
    "    'beamsize': 6,\n",
    "    'normalization_factor': 0.5,\n",
    "    'verbose': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_name_to_test_dataset = {\n",
    "    'default': test_default_dataset,\n",
    "    'extra': test_extra_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 627/627 [06:02<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "bs_params = [\n",
    "    (\"extra\", get_m1_bigger_model, \"m1_bigger/m1_bigger_v2__2023_11_12__02_27_14__0.13413_extra_l2_0_ls0_switch_1.pt\"),\n",
    "\n",
    "    # \"m1_smaller__m1_smaller_v2_2023_11_12_08_17_33_0_31223_default_l2_1e_05_ls0_02.pt.pkl\": 0.8835960067969484,\n",
    "    # \"m1_bigger__m1_bigger_v2__2023_11_11__15_53_07__0.13636_default_l2_0_ls0_switch_0.pt.pkl\": 0.8871590909090984,\n",
    "    # \"m1_bigger__m1_bigger_v2__2023_11_11__16_45_33__0.13721_extra_l2_0_ls0_switch_0.pt.pkl\": 0.8864383561643838,\n",
    "]\n",
    "\n",
    "\n",
    "for grid_name, model_getter, weights_f_name in bs_params:\n",
    "\n",
    "    bs_preds_path = os.path.join(\"../data/saved_beamsearch_results/\",\n",
    "                                f\"{weights_f_name.replace('/', '__')}.pkl\")\n",
    "    \n",
    "    if os.path.exists(bs_preds_path):\n",
    "        print(f\"Path {bs_preds_path} exists. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    bs_predictions = weights_to_raw_predictions(\n",
    "        grid_name = grid_name,\n",
    "        model_getter=model_getter,\n",
    "        weights_path = os.path.join(MODELS_ROOT, weights_f_name),\n",
    "        word_char_tokenizer=word_char_tokenizer,\n",
    "        dataset=grid_name_to_test_dataset[grid_name],\n",
    "        generator_ctor=BeamGenerator,\n",
    "        n_workers=4,\n",
    "        generator_kwargs=generator_kwargs\n",
    "    )\n",
    "\n",
    "    with open(bs_preds_path, 'wb') as f:\n",
    "        pickle.dump(bs_predictions, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9373, 9193)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(clean_default_test_predictions), sum(bool(el) for el in clean_default_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = get_vocab_set(os.path.join(DATA_ROOT, \"voc.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {4: 7404, 3: 1032, 2: 840, 1: 577, 0: 147})\n"
     ]
    }
   ],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# n_preds_in_line_dict = defaultdict(int)\n",
    "\n",
    "# for line in clean_test_predictions:\n",
    "#     n_preds_in_line_dict[len(line)] += 1\n",
    "\n",
    "# print(n_preds_in_line_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_preds_path = os.path.join(DATA_ROOT, \"test_raw_pred___best_model__2023_11_04__18_31_37__0.02530_default_switch_2.pt__best_model__2023_11_05__07_55_13__0.02516_extra_switch_2__with_pad_cutting.pt.pkl\")\n",
    "with open(old_preds_path, 'rb') as f:\n",
    "    old_preds_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_preds_list = remove_beamsearch_probs(old_preds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_preds_list_valid, old_preds_list_invalid = separate_out_vocab_all_crvs(old_preds_list, vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_name = \"default__m1_bigger_13679__m1_v2__14229___extra__14301___with_baseline__beam.csv\"\n",
    "# out_path = rf\"..\\data\\submissions\\{submission_name}\"\n",
    "# create_submission(clean_test_baseline_augmented, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id 1\n",
    "\n",
    "```\n",
    "grid_name_to_ranged_bs_model_preds_paths = {\n",
    "    'default': [\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__12_30_29__0.13121__greed_acc_0.86098__default_l2_0_ls0_switch_2.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__22_18_35__0.13542_default_l2_0_ls0_switch_1.pt.pkl\",\n",
    "        \"m1_v2__m1_v2__2023_11_09__10_36_02__0.14229_default_switch_0.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__00_39_33__0.13297_default_l2_0_ls0_switch_1.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__14_29_37__0.13679_default_l2_0_ls0_switch_0.pt.pkl\",\n",
    "        \n",
    "    ],\n",
    "    'extra': [\"m1_v2__m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt.pkl\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id 2\n",
    "\n",
    "```\n",
    "grid_name_to_ranged_bs_model_preds_paths = {\n",
    "    'default': [\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__12_30_29__0.13121__greed_acc_0.86098__default_l2_0_ls0_switch_2.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__22_18_35__0.13542_default_l2_0_ls0_switch_1.pt.pkl\",\n",
    "        \"m1_v2__m1_v2__2023_11_09__10_36_02__0.14229_default_switch_0.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__00_39_33__0.13297_default_l2_0_ls0_switch_1.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__14_29_37__0.13679_default_l2_0_ls0_switch_0.pt.pkl\",\n",
    "        \n",
    "    ],\n",
    "    'extra': [\n",
    "        \"m1_v2__m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__02_27_14__0.13413_extra_l2_0_ls0_switch_1.pt.pkl\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# должны ранжироваться по качесту beamsearch на валидации\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id 3\n",
    "\n",
    "```python\n",
    "grid_name_to_ranged_bs_model_preds_paths = {\n",
    "    'default': [\n",
    "        \"m1_bigger_m1_bigger_v2_2023_11_12_14_51_49_0_13115_greed_acc_0_86034.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__12_30_29__0.13121__greed_acc_0.86098__default_l2_0_ls0_switch_2.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__22_18_35__0.13542_default_l2_0_ls0_switch_1.pt.pkl\",\n",
    "        \"m1_v2__m1_v2__2023_11_09__10_36_02__0.14229_default_switch_0.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__00_39_33__0.13297_default_l2_0_ls0_switch_1.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__14_29_37__0.13679_default_l2_0_ls0_switch_0.pt.pkl\",\n",
    "        \n",
    "    ],\n",
    "    'extra': [\n",
    "        \"m1_v2__m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__02_27_14__0.13413_extra_l2_0_ls0_switch_1.pt.pkl\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# должны ранжироваться по качесту beamsearch на валидации\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_name_to_ranged_bs_model_preds_paths = {\n",
    "    'default': [\n",
    "        \"m1_bigger_m1_bigger_v2_2023_11_12_14_51_49_0_13115_greed_acc_0_86034.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__12_30_29__0.13121__greed_acc_0.86098__default_l2_0_ls0_switch_2.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__22_18_35__0.13542_default_l2_0_ls0_switch_1.pt.pkl\",\n",
    "        \"m1_v2__m1_v2__2023_11_09__10_36_02__0.14229_default_switch_0.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__00_39_33__0.13297_default_l2_0_ls0_switch_1.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_11__14_29_37__0.13679_default_l2_0_ls0_switch_0.pt.pkl\",\n",
    "        \n",
    "    ],\n",
    "    'extra': [\n",
    "        \"m1_v2__m1_v2__2023_11_09__17_47_40__0.14301_extra_l2_1e-05_switch_0.pt.pkl\",\n",
    "        \"m1_bigger__m1_bigger_v2__2023_11_12__02_27_14__0.13413_extra_l2_0_ls0_switch_1.pt.pkl\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# должны ранжироваться по качесту beamsearch на валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_idxs = test_default_dataset.grid_name_idxs\n",
    "extra_idxs = test_extra_dataset.grid_name_idxs \n",
    "\n",
    "grid_name_to_augmented_preds = {}\n",
    "\n",
    "for grid_name in ('default', 'extra'):\n",
    "    bs_pred_list = []\n",
    "\n",
    "    for f_name in grid_name_to_ranged_bs_model_preds_paths[grid_name]:\n",
    "        f_path = os.path.join(\"../data/saved_beamsearch_results/\", f_name)\n",
    "        with open(f_path, 'rb') as f:\n",
    "            bs_pred_list.append(pickle.load(f))\n",
    "        \n",
    "    bs_pred_list = [patch_wrong_prediction_shape(bs_preds) for bs_preds in bs_pred_list] \n",
    "    bs_pred_list = [remove_beamsearch_probs(bs_preds) for bs_preds in bs_pred_list]\n",
    "    bs_pred_list = [separate_out_vocab_all_crvs(bs_preds, vocab_set)[0] for bs_preds in bs_pred_list]\n",
    "\n",
    "\n",
    "    augmented_preds = bs_pred_list.pop(0)\n",
    "\n",
    "    while bs_pred_list:\n",
    "        augmented_preds = append_preds(augmented_preds, bs_pred_list.pop(0))\n",
    "\n",
    "    grid_name_to_augmented_preds[grid_name] = augmented_preds\n",
    "\n",
    "\n",
    "full_preds = merge_default_and_extra_preds(\n",
    "    grid_name_to_augmented_preds['default'],\n",
    "    grid_name_to_augmented_preds['extra'],\n",
    "    default_idxs,\n",
    "    extra_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {4: 8188, 3: 706, 2: 606, 1: 407, 0: 93})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "n_preds_in_line_dict = defaultdict(int)\n",
    "\n",
    "for line in full_preds:\n",
    "    n_preds_in_line_dict[len(line)] += 1\n",
    "\n",
    "print(n_preds_in_line_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_preds = None\n",
    "with open(r\"..\\data\\submissions\\sample_submission.csv\", 'r', encoding = 'utf-8') as f:\n",
    "    baseline_preds = f.read().splitlines()\n",
    "baseline_preds = [line.split(\",\") for line in augment_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_preds = append_preds(full_preds, baseline_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(full_preds,\n",
    "                  f\"../data/submissions/id3_with_baseline_without_old_preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_preds_augmentations = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
